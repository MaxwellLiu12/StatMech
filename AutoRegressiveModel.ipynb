{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dist\n",
    "import torch_scatter\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Lattice' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-332f6b832d50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mAutoregressiveModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDistribution\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \"\"\" Represent a generative model that can generate samples and evaluate log probabilities.\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mArgs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mlattice\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlattice\u001b[0m \u001b[0msystem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-332f6b832d50>\u001b[0m in \u001b[0;36mAutoregressiveModel\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \"\"\"\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlattice\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mLattice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnonlinearity\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Tanh'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAutoregressiveModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlattice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlattice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Lattice' is not defined"
     ]
    }
   ],
   "source": [
    "class AutoregressiveModel(nn.Module, dist.Distribution):\n",
    "    \"\"\" Represent a generative model that can generate samples and evaluate log probabilities.\n",
    "        \n",
    "        Args:\n",
    "        lattice: lattice system\n",
    "        features: a list of feature dimensions for all layers\n",
    "        nonlinearity: activation function to use \n",
    "        bias: whether to learn the bias\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lattice: Lattice, features, nonlinearity: str = 'Tanh', bias: bool = True):\n",
    "        super(AutoregressiveModel, self).__init__()\n",
    "        self.lattice = lattice\n",
    "        self.nodes = lattice.sites\n",
    "        self.max_depth = lattice.max_depth\n",
    "        self.features = features\n",
    "        dist.Distribution.__init__(self, event_shape=torch.Size([self.nodes, self.features[0]]))\n",
    "        self.has_rsample = True\n",
    "        #self.graph = self.lattice.graph\n",
    "        self.layers = nn.ModuleList()\n",
    "        for l in range(1, len(self.features)):\n",
    "            if l == 1: # the first layer should not have self loops\n",
    "                self.layers.append(GraphConv(self.lattice, self.features[0], self.features[1], bias, self_loop = False))\n",
    "            else: # remaining layers are normal\n",
    "                self.layers.append(nn.LayerNorm([self.features[l - 1]]))\n",
    "                self.layers.append(getattr(nn, nonlinearity)()) # activatioin layer\n",
    "                self.layers.append(GraphConv(self.lattice, self.features[l - 1], self.features[l], bias))\n",
    "\n",
    "    def update_graph(self, graph):\n",
    "        # update graph for all GraphConv layers\n",
    "        self.graph = graph\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, GraphConv):\n",
    "                layer.update_graph(graph)\n",
    "        return self\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = input\n",
    "        for layer in self.layers: # apply layers\n",
    "            output = layer(output)\n",
    "        return output # logits\n",
    "    \n",
    "    def log_prob(self, sample):\n",
    "        logits = self(sample) # forward pass to get logits\n",
    "        return torch.sum(sample * F.log_softmax(logits, dim=-1), (-2,-1))\n",
    "\n",
    "    def sampler(self, logits, dim=-1): # simplified from F.gumbel_softmax\n",
    "        gumbels = -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format).exponential_().log()\n",
    "        gumbels += logits.detach()\n",
    "        index = gumbels.max(dim, keepdim=True)[1]\n",
    "        return torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(dim, index, 1.0)\n",
    "\n",
    "    def _sample(self, sample_size: int, sampler = None):\n",
    "        if sampler is None: # if no sampler specified, use default\n",
    "            sampler = self.sampler\n",
    "        # create a list of tensors to cache layer-wise outputs\n",
    "        cache = [torch.zeros(self.max_depth * self.features[0], sample_size)]\n",
    "        depth_assignments = [self.layers[0].depth_assignment]\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, GraphConv): # for graph convolution layers\n",
    "                features = layer.out_features # features get updated\n",
    "                depths = layer.depth_assignment\n",
    "            cache.append(torch.zeros(self.nodes * features, sample_size))\n",
    "            depth_assignments.append(depths)\n",
    "        # cache established. start by sampling node 0.\n",
    "        # assuming global symmetry, node 0 is always sampled uniformly\n",
    "        cache[0][..., 0, :] = sampler(cache[0][..., 0, :])\n",
    "        # start autoregressive sampling\n",
    "        for j in range(1, self.max_depth + 1): # iterate through nodes 1:all\n",
    "            for l, layer in enumerate(self.layers):\n",
    "                if isinstance(layer, GraphConv): # for graph convolution layers\n",
    "                    if l==0: # first layer should forward from previous node\n",
    "                        cache[l + 1] += layer(cache[l], j - 1)#possibly change to not be in place operation\n",
    "                    else: # remaining layers forward from this node\n",
    "                        cache[l + 1] += layer(cache[l], j)\n",
    "                else: # for other layers, only update node j (other nodes not ready yet)\n",
    "                    src = layer(cache[l][..., [j], :])\n",
    "                    index = torch.tensor(j).view([1]*src.dim()).expand(src.size())\n",
    "                    cache[l + 1] = cache[l + 1].scatter(-2, index, src)#scatter incorrect\n",
    "            # the last cache hosts the logit, sample from it \n",
    "            cache[0][..., j, :] = sampler(cache[-1][..., j, :])\n",
    "        return cache # cache[0] hosts the sample\n",
    "    \n",
    "    def sample(self, sample_size=1):\n",
    "        with torch.no_grad():\n",
    "            cache = self._sample(sample_size)\n",
    "        return cache[0]\n",
    "    \n",
    "    def rsample(self, sample_size=1, tau=None, hard=False):\n",
    "        # reparametrized Gumbel sampling\n",
    "        if tau is None: # if temperature not given\n",
    "            tau = 1/(self.features[-1]-1) # set by the out feature dimension\n",
    "        cache = self._sample(sample_size, lambda x: F.gumbel_softmax(x, tau, hard))\n",
    "        return cache[0]\n",
    "\n",
    "    def sample_with_log_prob(self, sample_size=1):\n",
    "        cache = self._sample(sample_size)\n",
    "        sample = cache[0]\n",
    "        logits = cache[-1]\n",
    "        log_prob = torch.sum(sample * F.log_softmax(logits, dim=-1), (-2,-1))\n",
    "        return sample, log_prob"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
