{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConv(nn.Module):\n",
    "    \"\"\" Graph Convolution layer \n",
    "        \n",
    "        Args:\n",
    "        graph: tensor of shape [3, num_edges] \n",
    "               specifying (source, target, type) along each column\n",
    "        in_features: number of input features (per node)\n",
    "        out_features: number of output features (per node)\n",
    "        bias: whether to learn an edge-depenent bias\n",
    "        self_loop: whether to include self loops in message passing\n",
    "    \"\"\"\n",
    "    def __init__(self, lattice, in_features: int, out_features: int,\n",
    "                 bias: bool = True, self_loop: bool = True):\n",
    "        super(GraphConv, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        if bias:\n",
    "            self.bias = bias\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.edge_types = None\n",
    "        self.self_loop = self_loop\n",
    "        self.lattice_sites = lattice.sites\n",
    "        self.update_graph(lattice.graph)\n",
    "        self.conv_size = (self.lattice_sites * out_features, self.lattice_sites * in_features)\n",
    "\n",
    "    def update_graph(self, graph):\n",
    "        # update the graph, adding new linear maps if needed\n",
    "        if not self.self_loop:\n",
    "            graph = graph.remove_self_loops()#removes any self_loops according to boolean\n",
    "        self.graph = graph.expand_features(self.in_features, self.out_features)#expands the graph features\n",
    "        self.graph = self.graph.inverse_connections()\n",
    "        self.depth_assignment = graph.get_depth_assignment()\n",
    "        self.forwarding_graphs_init()\n",
    "        edge_types = self.graph.edge_types\n",
    "        if edge_types != self.edge_types:\n",
    "            self.weight = nn.Parameter(torch.Tensor(edge_types))\n",
    "            if self.bias is not None:\n",
    "                self.bias = nn.Parameter(torch.Tensor(edge_types, self.out_features))\n",
    "            self.reset_parameters()\n",
    "        self.edge_types = edge_types\n",
    "        return self\n",
    "    \n",
    "    def forwarding_graphs_init(self):\n",
    "        self.forwarding_graphs = []\n",
    "        for depth in self.depth_assignment:\n",
    "            self.forwarding_graphs.append(self.graph.select_connections(*depth))\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        bound = 1 / math.sqrt(self.lattice_sites * self.in_features)\n",
    "        nn.init.uniform_(self.weight, -bound, bound)\n",
    "        if self.bias is not None:\n",
    "            fan_in = self.in_features\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return 'edge_types={}, in_features={}, out_features={}, bias={}, self_loop={}'.format(\n",
    "            self.edge_types, self.in_features, self.out_features, self.bias is not None, self.self_loop)\n",
    "\n",
    "    def forward(self, input, depth = None):\n",
    "        input = input.flatten(1).t()\n",
    "        if depth == None:\n",
    "            signal, edge_type = self.graph.sparse_graph()\n",
    "            weights = torch.gather(self.weight, 0, edge_type)\n",
    "            conv = torch.sparse_coo_tensor(signal, weights, size = self.conv_size)\n",
    "            output = torch.sparse.mm(conv, input)\n",
    "        else:\n",
    "            signal, edge_type = self.forwarding_graphs[depth].sparse_graph()\n",
    "            weights = torch.gather(self.weight, 0, edge_type)\n",
    "            conv = torch.sparse_coo_tensor(signal, weights, size = self.conv_size)\n",
    "            output = torch.sparse.mm(conv, input)\n",
    "        output = output.t().unflatten(1, (self.lattice_sites, self.out_features))\n",
    "        #if self.bias:\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw input tensor([[[  0.,   1.],\n",
      "         [  2.,   3.],\n",
      "         [  4.,   5.],\n",
      "         [  6.,   7.],\n",
      "         [  8.,   9.],\n",
      "         [ 10.,  11.],\n",
      "         [ 12.,  13.],\n",
      "         [ 14.,  15.],\n",
      "         [ 16.,  17.],\n",
      "         [ 18.,  19.],\n",
      "         [ 20.,  21.],\n",
      "         [ 22.,  23.],\n",
      "         [ 24.,  25.],\n",
      "         [ 26.,  27.],\n",
      "         [ 28.,  29.],\n",
      "         [ 30.,  31.]],\n",
      "\n",
      "        [[ 32.,  33.],\n",
      "         [ 34.,  35.],\n",
      "         [ 36.,  37.],\n",
      "         [ 38.,  39.],\n",
      "         [ 40.,  41.],\n",
      "         [ 42.,  43.],\n",
      "         [ 44.,  45.],\n",
      "         [ 46.,  47.],\n",
      "         [ 48.,  49.],\n",
      "         [ 50.,  51.],\n",
      "         [ 52.,  53.],\n",
      "         [ 54.,  55.],\n",
      "         [ 56.,  57.],\n",
      "         [ 58.,  59.],\n",
      "         [ 60.,  61.],\n",
      "         [ 62.,  63.]],\n",
      "\n",
      "        [[ 64.,  65.],\n",
      "         [ 66.,  67.],\n",
      "         [ 68.,  69.],\n",
      "         [ 70.,  71.],\n",
      "         [ 72.,  73.],\n",
      "         [ 74.,  75.],\n",
      "         [ 76.,  77.],\n",
      "         [ 78.,  79.],\n",
      "         [ 80.,  81.],\n",
      "         [ 82.,  83.],\n",
      "         [ 84.,  85.],\n",
      "         [ 86.,  87.],\n",
      "         [ 88.,  89.],\n",
      "         [ 90.,  91.],\n",
      "         [ 92.,  93.],\n",
      "         [ 94.,  95.]],\n",
      "\n",
      "        [[ 96.,  97.],\n",
      "         [ 98.,  99.],\n",
      "         [100., 101.],\n",
      "         [102., 103.],\n",
      "         [104., 105.],\n",
      "         [106., 107.],\n",
      "         [108., 109.],\n",
      "         [110., 111.],\n",
      "         [112., 113.],\n",
      "         [114., 115.],\n",
      "         [116., 117.],\n",
      "         [118., 119.],\n",
      "         [120., 121.],\n",
      "         [122., 123.],\n",
      "         [124., 125.],\n",
      "         [126., 127.]],\n",
      "\n",
      "        [[128., 129.],\n",
      "         [130., 131.],\n",
      "         [132., 133.],\n",
      "         [134., 135.],\n",
      "         [136., 137.],\n",
      "         [138., 139.],\n",
      "         [140., 141.],\n",
      "         [142., 143.],\n",
      "         [144., 145.],\n",
      "         [146., 147.],\n",
      "         [148., 149.],\n",
      "         [150., 151.],\n",
      "         [152., 153.],\n",
      "         [154., 155.],\n",
      "         [156., 157.],\n",
      "         [158., 159.]]])\n",
      "transformed input tensor([[  0.,  32.,  64.,  96., 128.],\n",
      "        [  1.,  33.,  65.,  97., 129.],\n",
      "        [  2.,  34.,  66.,  98., 130.],\n",
      "        [  3.,  35.,  67.,  99., 131.],\n",
      "        [  4.,  36.,  68., 100., 132.],\n",
      "        [  5.,  37.,  69., 101., 133.],\n",
      "        [  6.,  38.,  70., 102., 134.],\n",
      "        [  7.,  39.,  71., 103., 135.],\n",
      "        [  8.,  40.,  72., 104., 136.],\n",
      "        [  9.,  41.,  73., 105., 137.],\n",
      "        [ 10.,  42.,  74., 106., 138.],\n",
      "        [ 11.,  43.,  75., 107., 139.],\n",
      "        [ 12.,  44.,  76., 108., 140.],\n",
      "        [ 13.,  45.,  77., 109., 141.],\n",
      "        [ 14.,  46.,  78., 110., 142.],\n",
      "        [ 15.,  47.,  79., 111., 143.],\n",
      "        [ 16.,  48.,  80., 112., 144.],\n",
      "        [ 17.,  49.,  81., 113., 145.],\n",
      "        [ 18.,  50.,  82., 114., 146.],\n",
      "        [ 19.,  51.,  83., 115., 147.],\n",
      "        [ 20.,  52.,  84., 116., 148.],\n",
      "        [ 21.,  53.,  85., 117., 149.],\n",
      "        [ 22.,  54.,  86., 118., 150.],\n",
      "        [ 23.,  55.,  87., 119., 151.],\n",
      "        [ 24.,  56.,  88., 120., 152.],\n",
      "        [ 25.,  57.,  89., 121., 153.],\n",
      "        [ 26.,  58.,  90., 122., 154.],\n",
      "        [ 27.,  59.,  91., 123., 155.],\n",
      "        [ 28.,  60.,  92., 124., 156.],\n",
      "        [ 29.,  61.,  93., 125., 157.],\n",
      "        [ 30.,  62.,  94., 126., 158.],\n",
      "        [ 31.,  63.,  95., 127., 159.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00],\n",
       "         [6.0000e+00, 6.0000e+00, 6.0000e+00, 6.0000e+00],\n",
       "         [1.5000e+01, 1.5000e+01, 1.5000e+01, 1.5000e+01],\n",
       "         [2.8000e+01, 2.8000e+01, 2.8000e+01, 2.8000e+01],\n",
       "         [4.5000e+01, 4.5000e+01, 4.5000e+01, 4.5000e+01],\n",
       "         [6.6000e+01, 6.6000e+01, 6.6000e+01, 6.6000e+01],\n",
       "         [9.1000e+01, 9.1000e+01, 9.1000e+01, 9.1000e+01],\n",
       "         [1.2000e+02, 1.2000e+02, 1.2000e+02, 1.2000e+02],\n",
       "         [1.5300e+02, 1.5300e+02, 1.5300e+02, 1.5300e+02],\n",
       "         [1.9000e+02, 1.9000e+02, 1.9000e+02, 1.9000e+02],\n",
       "         [2.3100e+02, 2.3100e+02, 2.3100e+02, 2.3100e+02],\n",
       "         [2.7600e+02, 2.7600e+02, 2.7600e+02, 2.7600e+02],\n",
       "         [3.2500e+02, 3.2500e+02, 3.2500e+02, 3.2500e+02],\n",
       "         [3.7800e+02, 3.7800e+02, 3.7800e+02, 3.7800e+02],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
       "\n",
       "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [6.5000e+01, 6.5000e+01, 6.5000e+01, 6.5000e+01],\n",
       "         [1.3400e+02, 1.3400e+02, 1.3400e+02, 1.3400e+02],\n",
       "         [2.0700e+02, 2.0700e+02, 2.0700e+02, 2.0700e+02],\n",
       "         [2.8400e+02, 2.8400e+02, 2.8400e+02, 2.8400e+02],\n",
       "         [3.6500e+02, 3.6500e+02, 3.6500e+02, 3.6500e+02],\n",
       "         [4.5000e+02, 4.5000e+02, 4.5000e+02, 4.5000e+02],\n",
       "         [5.3900e+02, 5.3900e+02, 5.3900e+02, 5.3900e+02],\n",
       "         [6.3200e+02, 6.3200e+02, 6.3200e+02, 6.3200e+02],\n",
       "         [7.2900e+02, 7.2900e+02, 7.2900e+02, 7.2900e+02],\n",
       "         [8.3000e+02, 8.3000e+02, 8.3000e+02, 8.3000e+02],\n",
       "         [9.3500e+02, 9.3500e+02, 9.3500e+02, 9.3500e+02],\n",
       "         [1.0440e+03, 1.0440e+03, 1.0440e+03, 1.0440e+03],\n",
       "         [1.1570e+03, 1.1570e+03, 1.1570e+03, 1.1570e+03],\n",
       "         [1.2740e+03, 1.2740e+03, 1.2740e+03, 1.2740e+03],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
       "\n",
       "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.2900e+02, 1.2900e+02, 1.2900e+02, 1.2900e+02],\n",
       "         [2.6200e+02, 2.6200e+02, 2.6200e+02, 2.6200e+02],\n",
       "         [3.9900e+02, 3.9900e+02, 3.9900e+02, 3.9900e+02],\n",
       "         [5.4000e+02, 5.4000e+02, 5.4000e+02, 5.4000e+02],\n",
       "         [6.8500e+02, 6.8500e+02, 6.8500e+02, 6.8500e+02],\n",
       "         [8.3400e+02, 8.3400e+02, 8.3400e+02, 8.3400e+02],\n",
       "         [9.8700e+02, 9.8700e+02, 9.8700e+02, 9.8700e+02],\n",
       "         [1.1440e+03, 1.1440e+03, 1.1440e+03, 1.1440e+03],\n",
       "         [1.3050e+03, 1.3050e+03, 1.3050e+03, 1.3050e+03],\n",
       "         [1.4700e+03, 1.4700e+03, 1.4700e+03, 1.4700e+03],\n",
       "         [1.6390e+03, 1.6390e+03, 1.6390e+03, 1.6390e+03],\n",
       "         [1.8120e+03, 1.8120e+03, 1.8120e+03, 1.8120e+03],\n",
       "         [1.9890e+03, 1.9890e+03, 1.9890e+03, 1.9890e+03],\n",
       "         [2.1700e+03, 2.1700e+03, 2.1700e+03, 2.1700e+03],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
       "\n",
       "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.9300e+02, 1.9300e+02, 1.9300e+02, 1.9300e+02],\n",
       "         [3.9000e+02, 3.9000e+02, 3.9000e+02, 3.9000e+02],\n",
       "         [5.9100e+02, 5.9100e+02, 5.9100e+02, 5.9100e+02],\n",
       "         [7.9600e+02, 7.9600e+02, 7.9600e+02, 7.9600e+02],\n",
       "         [1.0050e+03, 1.0050e+03, 1.0050e+03, 1.0050e+03],\n",
       "         [1.2180e+03, 1.2180e+03, 1.2180e+03, 1.2180e+03],\n",
       "         [1.4350e+03, 1.4350e+03, 1.4350e+03, 1.4350e+03],\n",
       "         [1.6560e+03, 1.6560e+03, 1.6560e+03, 1.6560e+03],\n",
       "         [1.8810e+03, 1.8810e+03, 1.8810e+03, 1.8810e+03],\n",
       "         [2.1100e+03, 2.1100e+03, 2.1100e+03, 2.1100e+03],\n",
       "         [2.3430e+03, 2.3430e+03, 2.3430e+03, 2.3430e+03],\n",
       "         [2.5800e+03, 2.5800e+03, 2.5800e+03, 2.5800e+03],\n",
       "         [2.8210e+03, 2.8210e+03, 2.8210e+03, 2.8210e+03],\n",
       "         [3.0660e+03, 3.0660e+03, 3.0660e+03, 3.0660e+03],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
       "\n",
       "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [2.5700e+02, 2.5700e+02, 2.5700e+02, 2.5700e+02],\n",
       "         [5.1800e+02, 5.1800e+02, 5.1800e+02, 5.1800e+02],\n",
       "         [7.8300e+02, 7.8300e+02, 7.8300e+02, 7.8300e+02],\n",
       "         [1.0520e+03, 1.0520e+03, 1.0520e+03, 1.0520e+03],\n",
       "         [1.3250e+03, 1.3250e+03, 1.3250e+03, 1.3250e+03],\n",
       "         [1.6020e+03, 1.6020e+03, 1.6020e+03, 1.6020e+03],\n",
       "         [1.8830e+03, 1.8830e+03, 1.8830e+03, 1.8830e+03],\n",
       "         [2.1680e+03, 2.1680e+03, 2.1680e+03, 2.1680e+03],\n",
       "         [2.4570e+03, 2.4570e+03, 2.4570e+03, 2.4570e+03],\n",
       "         [2.7500e+03, 2.7500e+03, 2.7500e+03, 2.7500e+03],\n",
       "         [3.0470e+03, 3.0470e+03, 3.0470e+03, 3.0470e+03],\n",
       "         [3.3480e+03, 3.3480e+03, 3.3480e+03, 3.3480e+03],\n",
       "         [3.6530e+03, 3.6530e+03, 3.6530e+03, 3.6530e+03],\n",
       "         [3.9620e+03, 3.9620e+03, 3.9620e+03, 3.9620e+03],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lattice = model.Lattice(4,2)\n",
    "test_layer = GraphConv(test_lattice, 2, 4, self_loop = False)\n",
    "test_input = torch.arange(160, dtype = torch.float).view(5, 16, 2)\n",
    "test_layer(test_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
