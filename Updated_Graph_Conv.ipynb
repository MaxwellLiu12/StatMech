{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConv(nn.Module):\n",
    "    \"\"\" Graph Convolution layer \n",
    "        \n",
    "        Args:\n",
    "        graph: tensor of shape [3, num_edges] \n",
    "               specifying (source, target, type) along each column\n",
    "        in_features: number of input features (per node)\n",
    "        out_features: number of output features (per node)\n",
    "        bias: whether to learn an edge-depenent bias\n",
    "        self_loop: whether to include self loops in message passing\n",
    "    \"\"\"\n",
    "    def __init__(self, lattice, in_features: int, out_features: int,\n",
    "                 bias: bool = True, self_loop: bool = True):\n",
    "        super(GraphConv, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        if bias:\n",
    "            self.bias = bias\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.edge_types = None\n",
    "        self.self_loop = self_loop\n",
    "        self.lattice_sites = lattice.sites\n",
    "        self.update_graph(lattice.graph)\n",
    "        self.conv_size = (self.lattice_sites * out_features, self.lattice_sites * in_features)\n",
    "\n",
    "    def update_graph(self, graph):\n",
    "        # update the graph, adding new linear maps if needed\n",
    "        if not self.self_loop:\n",
    "            graph = graph.remove_self_loops()#removes any self_loops according to boolean\n",
    "        self.graph = graph.expand_features(self.in_features, self.out_features)#expands the graph features\n",
    "        self.graph = self.graph.inverse_connections()\n",
    "        self.depth_assignment = self.graph.get_depth_assignment()\n",
    "        self.forwarding_graphs_init()\n",
    "        edge_types = self.graph.edge_types\n",
    "        if edge_types != self.edge_types:\n",
    "            self.weight = nn.Parameter(torch.Tensor(edge_types))\n",
    "            if self.bias is not None:\n",
    "                self.bias = nn.Parameter(torch.Tensor(edge_types, self.out_features))\n",
    "            self.reset_parameters()\n",
    "        self.edge_types = edge_types\n",
    "        return self\n",
    "    \n",
    "    def forwarding_graphs_init(self):\n",
    "        self.forwarding_graphs = []\n",
    "        for depth in self.depth_assignment:\n",
    "            self.forwarding_graphs.append(self.graph.select_connections(*depth))\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        bound = 1 / math.sqrt(self.lattice_sites * self.in_features)\n",
    "        nn.init.uniform_(self.weight, -bound, bound)\n",
    "        if self.bias is not None:\n",
    "            fan_in = self.in_features\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return 'edge_types={}, in_features={}, out_features={}, bias={}, self_loop={}'.format(\n",
    "            self.edge_types, self.in_features, self.out_features, self.bias is not None, self.self_loop)\n",
    "\n",
    "    def forward(self, input, depth = None):\n",
    "        input = input.flatten(1).t()\n",
    "        if depth == None:\n",
    "            signal, edge_type = self.graph.sparse_graph()\n",
    "            weights = torch.gather(self.weight, 0, edge_type)\n",
    "            conv = torch.sparse_coo_tensor(signal, weights, size = self.conv_size)\n",
    "            output = torch.sparse.mm(conv, input)\n",
    "        else:\n",
    "            signal, edge_type = self.forwarding_graphs[depth].sparse_graph()\n",
    "            weights = torch.gather(self.weight, 0, edge_type)\n",
    "            conv = torch.sparse_coo_tensor(signal, weights, size = self.conv_size)\n",
    "            output = torch.sparse.mm(conv, input)\n",
    "        output = output.t().unflatten(1, (self.lattice_sites, self.out_features))\n",
    "        #if self.bias:\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59], [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27], [4, 5, 6, 7, 8, 9, 10, 11], [0, 1, 2, 3]]\n"
     ]
    }
   ],
   "source": [
    "test_lattice = model.Lattice(4,2)\n",
    "test_layer = GraphConv(test_lattice, 2, 4, self_loop = False)\n",
    "test_input = torch.arange(5 * 16 * 2, dtype = torch.float).view(5, 16, 2)\n",
    "test_layer(test_input)\n",
    "print(test_layer.depth_assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
